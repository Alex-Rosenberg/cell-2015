{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/gs/vol3/software/modules-sw-python/2.7.3/pandas/0.14.0/Linux/RHEL6/x86_64/lib/python2.7/site-packages/pandas/io/excel.py:626: UserWarning: Installed openpyxl is not supported at this time. Use >=1.6.1 and <2.0.0.\n",
      "  .format(openpyxl_compat.start_ver, openpyxl_compat.stop_ver))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import scipy.stats\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import dnatools\n",
    "from plot_tools import simpleaxis, plot_splicing_histogram\n",
    "import re\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "\n",
    "# Plotting Params:\n",
    "rc('mathtext', default='regular')\n",
    "fsize=14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make directory to save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultsdir = '../results/N2_Splice_Site_Analysis/'\n",
    "if not os.path.exists(resultsdir):\n",
    "    os.makedirs(resultsdir)\n",
    "figdir = '../figures/N2_Splice_Site_Analysis/'\n",
    "if not os.path.exists(figdir):\n",
    "    os.makedirs(figdir)\n",
    "    \n",
    "#Choose if you want to actually save the plots:\n",
    "SAVEFIGS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sio.loadmat('../data/Reads.mat')\n",
    "\n",
    "# A5SS\n",
    "A5SS_data = data['A5SS']\n",
    "A5SS_data = np.array(A5SS_data.todense())\n",
    "# Get minigenes with reads\n",
    "A5SS_nn = find(A5SS_data.sum(axis=1))\n",
    "A5SS_data = A5SS_data[A5SS_nn]\n",
    "A5SS_data = A5SS_data/A5SS_data.sum(axis=1)[:,newaxis]\n",
    "A5SS_seqs = pd.read_csv('../data/A5SS_Seqs.csv',index_col=0).Seq[A5SS_nn]\n",
    "\n",
    "# A3SS\n",
    "A3SS_data = data['A3SS']\n",
    "# Only look at SA_1 usage:\n",
    "A3SS_data = np.array(A3SS_data[:,235].todense()).reshape(-1)/np.array(A3SS_data.sum(axis=1),dtype=np.float64).reshape(-1)\n",
    "# Get minigenes with reads\n",
    "A3SS_nn = find(pd.notnull(A3SS_data))\n",
    "A3SS_data = A3SS_data[A3SS_nn]\n",
    "A3SS_seqs = pd.read_csv('../data/A3SS_Seqs.csv',index_col=0).Seq[A3SS_nn]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a web logo for splice donors in the A5SS library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the overall base frequence in the randomized regions. Unforunately, due to dna synthesis biases, it is not 25/25/25/25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_freq_all = {}\n",
    "for b in dnatools.bases:\n",
    "    base_freq_all[b] = pd.Series(A5SS_seqs).str.slice(7,32).str.count(b).sum() + pd.Series(A5SS_seqs).str.slice(50,75).str.count(b).sum()\n",
    "base_freq_all = pd.Series(base_freq_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the base frequencies at each position (-3 to +6) at splice donors used over 10% of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sd_base_freqs = {}\n",
    "for b in range(9):\n",
    "    sd_base_freqs[b] = pd.Series(A5SS_seqs[find(A5SS_data[:,11]>0.1)]).groupby(pd.Series(A5SS_seqs[find(A5SS_data[:,11]>0.1)]).str.slice(7+b,7+1+b)).size()\n",
    "    for i in range(12,32-6)+range(54,75-6):\n",
    "        sd_base_freqs[b] += pd.Series(A5SS_seqs[find(A5SS_data[:,i]>0.1)]).groupby(pd.Series(A5SS_seqs[find(A5SS_data[:,i]>0.1)]).str.slice(i-4+b,i-3+b)).size()\n",
    "    print b,\n",
    "sd_base_freqs = pd.DataFrame(sd_base_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the relative base frequencies of splice donors to all randomized sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd_to_random_ratio = (sd_base_freqs.fillna(0).T/base_freq_all).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sd_to_random_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize each position (column) so it sums to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_base_ratio = (sd_to_random_ratio/sd_to_random_ratio.T.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_base_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there might be a better way to do this, but I'm simply going to sample sequnces from the normalized_base_ratio computed above and submit those sequences to the web logo program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_prob_dict = normalized_base_ratio.to_dict()\n",
    "sampled_bases = {}\n",
    "for pos in range(9):\n",
    "    sampled_bases[pos] = []\n",
    "    for b in 'ACGT':\n",
    "        sampled_bases[pos] += [b]*int(round(base_prob_dict[pos][b]*1000000))\n",
    "    sampled_bases[pos] = np.array(sampled_bases[pos])\n",
    "def make_sd():\n",
    "    sd = ''\n",
    "    for i in range(9):\n",
    "        sd += np.random.choice(sampled_bases[i])\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(resultsdir+'Generated_SD.txt','w')\n",
    "for i in range(10000):\n",
    "    f.write(make_sd()+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went to the weblogo website (http://weblogo.berkeley.edu/logo.cgi) and uploaded the file from above. I checked the frequency plot setting. I then downloaded and saved the file to the location below. In the actual paper figure, I drew a line at the exon-intron boundary (G|GT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weblogo=matplotlib.image.imread(figdir+'Library_SD_Normalized_Frequencies.png')\n",
    "imshow(weblogo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a web logo for splice acceptors in the A3SS library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis here is the same as for the A5SS library, so I haven't annotated all the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get each randomized region\n",
    "r1 = pd.Series(A3SS_seqs).str.slice(0,25)\n",
    "r2 = pd.Series(A3SS_seqs).str.slice(-25)\n",
    "Y = data['A3SS'][A3SS_nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_freq_all = {}\n",
    "for b in dnatools.bases:\n",
    "    base_freq_all[b] = r1.str.count(b).sum() + r2.str.count(b).sum()\n",
    "base_freq_all = pd.Series(base_freq_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find all new SA in the two randomized regions . Then calculate the frequency of each base at each position\n",
    "sa_base_freqs = {}\n",
    "for p in range(15,26):\n",
    "    inds = find(np.array(Y[:,189+24+25+p].todense())>0)\n",
    "    for b in range(0,min(p+5,25)):\n",
    "        try:\n",
    "            sa_base_freqs[b-p] = sa_base_freqs[b-p].add(r2[inds].groupby(r2[inds].str.slice(b,b+1)).size(),fill_value=0)\n",
    "        except:\n",
    "            sa_base_freqs[b-p] = r2[inds].groupby(r2[inds].str.slice(b,b+1)).size()\n",
    "for p in range(15,26):\n",
    "    inds = find(np.array(Y[:,189+p].todense())>0)\n",
    "    for b in range(0,min(p+5,25)):\n",
    "        try:\n",
    "            sa_base_freqs[b-p] = sa_base_freqs[b-p].add(r2[inds].groupby(r1[inds].str.slice(b,b+1)).size(),fill_value=0)\n",
    "        except:\n",
    "            sa_base_freqs[b-p] = r2[inds].groupby(r1[inds].str.slice(b,b+1)).size()\n",
    "sa_base_freqs = pd.DataFrame(sa_base_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sa_to_random_ratio = (sa_base_freqs.fillna(0).T/base_freq_all).T\n",
    "normalized_base_ratio = (sa_to_random_ratio/sa_to_random_ratio.T.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_prob_dict = normalized_base_ratio.to_dict()\n",
    "sampled_bases = {}\n",
    "for pos in range(-25,5):\n",
    "    sampled_bases[pos] = []\n",
    "    for b in 'ACGT':\n",
    "        sampled_bases[pos] += [b]*int(round(base_prob_dict[pos][b]*1000000))\n",
    "    sampled_bases[pos] = np.array(sampled_bases[pos])\n",
    "def make_sa():\n",
    "    sa = ''\n",
    "    for i in range(-25,5):\n",
    "        sa += np.random.choice(sampled_bases[i])\n",
    "    return sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(resultsdir+'Generated_SA.txt','w')\n",
    "for i in range(10000):\n",
    "    f.write(make_sa()+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went to the weblogo website (http://weblogo.berkeley.edu/logo.cgi) and uploaded the SA file from above. I checked the frequency plot setting. For the figure in the paper, I added a line between the intron-exon boundary (AG|G)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weblogo=matplotlib.image.imread(figdir+'Library_SA_Normalized_Frequencies.png')\n",
    "imshow(weblogo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the location of new splice sites in the randomized regions of the A5SS library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each new splice donor position in a randomized region, we want to calculate the mean usage. We exclude positions within 3nt of the random/fixed sequence boundaries. Here is the resulting plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  \n",
    "fig = figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.tick_params(labelsize=fsize)\n",
    "simpleaxis(ax)\n",
    "reads = log10(A5SS_data.mean(axis=0))\n",
    "reads[0:10]=nan\n",
    "reads[29:53]=nan\n",
    "reads[72:] = nan\n",
    "axis([0,80,-3.5,-1.9])\n",
    "ax.set_yticks(arange(-3.5,-1.99,0.5))\n",
    "ax.set_yticklabels(['$10^{-3.5}$','$10^{-3}$','$10^{-2.5}$','$10^{-2}$'],ha='right')\n",
    "ax.set_xticks(arange(0,100,25))\n",
    "y = (np.concatenate((reads[10:29],reads[53:72])))\n",
    "A = np.vstack([np.array(range(10,29)+range(53,72)), np.ones(38)]).T\n",
    "m,c = np.linalg.lstsq(A,y)[0]\n",
    "pred = arange(80)*m+c\n",
    "plot(arange(80),pred,'gray',linewidth=2)\n",
    "plot(reads,'o',markersize=4,color='b')\n",
    "\n",
    "ax.set_ylabel('Mean $SD_{NEW}$ Usage',fontsize=fsize)\n",
    "ax.set_xlabel('$SD_{NEW}$ Position\\n(Relative to $SD_1$)',fontsize=fsize)\n",
    "\n",
    "p,r = scipy.stats.pearsonr(np.concatenate((reads[10:29],reads[53:72])),np.concatenate((pred[10:29],pred[53:72])))\n",
    "ax.text(79,-2.25,'$\\it{p}$-pearson=%.2f' %p,ha='right',fontsize=fsize)\n",
    "ax.text(79,-2.5,'$\\it{P}$=%.1e' %r,ha='right',fontsize=fsize)\n",
    "\n",
    "if SAVEFIGS:\n",
    "    filename = 'Splice_Counts_vs_Distance'\n",
    "    fig.savefig(figdir+filename+'.png', bbox_inches='tight',dpi=300)\n",
    "    fig.savefig(figdir+filename+'.eps', bbox_inches='tight',dpi=200)\n",
    "    fig.savefig(figdir+filename+'.pdf', bbox_inches='tight',dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see if the slope within each randomized region is statistically significant. This is checked in the (hacky) code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  \n",
    "fig = figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.tick_params(labelsize=fsize)\n",
    "simpleaxis(ax)\n",
    "\n",
    "tb_start_x = 110\n",
    "tb_start_y = -3.25\n",
    "\n",
    "start = 10\n",
    "end = 29\n",
    "reads = log10(A5SS_data.mean(axis=0))\n",
    "reads[0:start]=nan\n",
    "reads[end:]=nan\n",
    "axis([0,80,-3.5,-1.9])\n",
    "ax.set_yticks(arange(-3.5,-1.99,0.5))\n",
    "ax.set_yticklabels(['$10^{-3.5}$','$10^{-3}$','$10^{-2.5}$','$10^{-2}$'],ha='right')\n",
    "ax.set_xticks(arange(0,100,25))\n",
    "y = reads[start:end]\n",
    "A = np.vstack([range(start,end), np.ones(19)]).T\n",
    "m,c = np.linalg.lstsq(A,y)[0]\n",
    "pred = arange(start,end)*m+c\n",
    "plot(arange(start,end),pred,'red',linewidth=2)\n",
    "plot(reads,'o',markersize=4,color='b')\n",
    "r,p = scipy.stats.pearsonr(reads[start:end],pred)\n",
    "ax.text(tb_start_x,tb_start_y,'Intercept' %r,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x,tb_start_y+0.25,'Slope' %r,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x,tb_start_y+0.5,'$\\it{P}$' %r,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x,tb_start_y+0.75,'$\\it{p}$-pearson' ,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+40,tb_start_y+1,'Region 1 Fit',ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+40,tb_start_y,'%0.4f' %c,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+40,tb_start_y+0.25,'%0.4f' %m,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+40,tb_start_y+0.75,'%0.4f' %r,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+40,tb_start_y+0.5,'%0.4f' %p,ha='center',fontsize=fsize)\n",
    "\n",
    "start = 53\n",
    "end = 72\n",
    "reads = log10(A5SS_data.mean(axis=0))\n",
    "reads[0:start]=nan\n",
    "reads[end:]=nan\n",
    "axis([0,80,-3.5,-1.9])\n",
    "ax.set_yticks(arange(-3.5,-1.99,0.5))\n",
    "ax.set_yticklabels(['$10^{-3.5}$','$10^{-3}$','$10^{-2.5}$','$10^{-2}$'],ha='right')\n",
    "ax.set_xticks(arange(0,100,25))\n",
    "y = reads[start:end]\n",
    "A = np.vstack([range(start,end), np.ones(19)]).T\n",
    "m,c = np.linalg.lstsq(A,y)[0]\n",
    "pred = arange(start,end)*m+c\n",
    "plot(arange(start,end),pred,'red',linewidth=2)\n",
    "plot(reads,'o',markersize=4,color='b')\n",
    "r,p = scipy.stats.pearsonr(reads[start:end],pred)\n",
    "ax.text(tb_start_x+80,tb_start_y+1,'Region 2 Fit',ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+80,tb_start_y,'%0.4f' %c,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+80,tb_start_y+0.25,'%0.4f' %m,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+80,tb_start_y+0.75,'%0.4f' %r,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+80,tb_start_y+0.5,'%0.4f' %p,ha='center',fontsize=fsize)\n",
    "\n",
    "reads = log10(A5SS_data.mean(axis=0))\n",
    "reads[0:10]=nan\n",
    "reads[29:53]=nan\n",
    "reads[72:] = nan\n",
    "axis([0,80,-3.5,-1.9])\n",
    "ax.set_yticks(arange(-3.5,-1.99,0.5))\n",
    "ax.set_yticklabels(['$10^{-3.5}$','$10^{-3}$','$10^{-2.5}$','$10^{-2}$'],ha='right')\n",
    "ax.set_xticks(arange(0,100,25))\n",
    "y = (np.concatenate((reads[10:29],reads[53:72])))\n",
    "A = np.vstack([np.array(range(10,29)+range(53,72)), np.ones(38)]).T\n",
    "m,c = np.linalg.lstsq(A,y)[0]\n",
    "pred = arange(80)*m+c\n",
    "plot(arange(80),pred,'gray',linewidth=2)\n",
    "plot(reads,'o',markersize=4,color='b')\n",
    "r,p = scipy.stats.pearsonr(np.concatenate((reads[10:29],reads[53:72])),\n",
    "                           np.concatenate((pred[10:29],pred[53:72])))\n",
    "\n",
    "ax.text(tb_start_x+120,tb_start_y+1,'Combined Fit',ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+120,tb_start_y,'%0.4f' %c,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+120,tb_start_y+0.25,'%0.4f' %m,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+120,tb_start_y+0.75,'%0.4f' %r,ha='center',fontsize=fsize)\n",
    "ax.text(tb_start_x+120,tb_start_y+0.5,'%1.2e' %p,ha='center',fontsize=fsize)\n",
    "\n",
    "ax.set_ylabel('Mean $SD_{NEW}$ Usage',fontsize=fsize)\n",
    "ax.set_xlabel('$SD_{NEW}$ Position\\n(Relative to $SD_1$)',fontsize=fsize)\n",
    "\n",
    "if SAVEFIGS:\n",
    "    filename = 'Splice_Counts_vs_Distance_Each_Region'\n",
    "    fig.savefig(figdir+filename+'.png', bbox_inches='tight',dpi=300)\n",
    "    fig.savefig(figdir+filename+'.eps', bbox_inches='tight',dpi=200)\n",
    "    fig.savefig(figdir+filename+'.pdf', bbox_inches='tight',dpi=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate the total number of reads at new splice donors in each randomized regions (not including positions within 3nt of the random/fixed sequence edges):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SD_new_reads_R1 = np.array(data['A5SS'].sum(axis=0)).flatten()[10:29].sum()\n",
    "SD_new_reads_R2 = np.array(data['A5SS'].sum(axis=0)).flatten()[53:72].sum()\n",
    "print SD_new_reads_R1,SD_new_reads_R2,SD_new_reads_R1/SD_new_reads_R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branch Point Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the consensus branch point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "branch_point_consensus = re.compile('[CT]T[AG]A[CT]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of consensus branch points at every position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "branch_points_by_position = {}\n",
    "for i in range(21):\n",
    "    branch_points_by_position[i] = r1.str.slice(0+i,5+i).str.contains(branch_point_consensus).sum()\n",
    "branch_points_by_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the mean usage of SA1 depending on the location (or lack) of a consensus branch point. We'll first define a function to find the position of a consensus branch point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regex_pos(reg,cur_str):\n",
    "    # Get the most 3' location of the branch point\n",
    "    # Return -1 if not found\n",
    "    pos = -1\n",
    "    for m in reg.finditer(cur_str):\n",
    "        pos = m.start()\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group SA_0 by BP position and get means, num introns\n",
    "grouped = pd.Series(A3SS_data).groupby(r1.apply(lambda x:regex_pos(branch_point_consensus,x)).values)\n",
    "bp_pos_data = grouped.aggregate({'Mean':mean,'Size':lambda x:pd.notnull(x).sum()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fsize=18\n",
    "\n",
    "fig = figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot([-1,22.5],[bp_pos_data.Mean[-1]]*2,linewidth=2,color='r',label='No Consensus BP')\n",
    "bp_pos_data.Mean[1:].plot(ax=ax,marker='o',linewidth=2,label='Consensus BP')\n",
    "axis([0,22.5,0,0.2])\n",
    "ax.set_xticks(range(-1,22,5));\n",
    "ax.set_xticklabels(range(-21-18-1,-18,5));\n",
    "ax.tick_params(labelsize=fsize)\n",
    "ax.set_xlabel('Branchpoint Position\\n(Relative to $SA_1$)',fontsize=fsize)\n",
    "ax.set_ylabel('$SA_1$ Usage (Fraction)',fontsize=fsize)\n",
    "#leg = legend(['No BP'],numpoints=1,bbox_to_anchor=(0.8,1),fontsize=fsize)\n",
    "#leg.get_frame().set_alpha(0)\n",
    "#ax.text(1,0.01,'No Consenseus BP',fontsize=fsize)\n",
    "ax.grid('off')\n",
    "simpleaxis(ax)\n",
    "\n",
    "if SAVEFIGS:\n",
    "    filename = 'Branch_point_position'\n",
    "    fig.savefig(figdir+filename+'.png', bbox_inches='tight',dpi=300)\n",
    "    fig.savefig(figdir+filename+'.eps', bbox_inches='tight',dpi=200)\n",
    "    fig.savefig(figdir+filename+'.pdf', bbox_inches='tight',dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether distance dependence on BP is due to AG exclusion zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible explanation for the distance dependence is the insertion of AG dinucleotides. AGs tend to be highly depleted between the branch point and the actual splice acceptor. People have speculated that the first AG after the branch point will be spliced. So let's check if that explains the distance dependence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regex_pos_ag(reg,cur_str):\n",
    "    # Get the most 3' location of the branch point\n",
    "    # Return -1 if not found\n",
    "    pos = -1\n",
    "    for m in reg.finditer(cur_str):\n",
    "        pos = m.start()\n",
    "    AG_found = cur_str[pos:].find('AG')>=0\n",
    "    return AG_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AG_inserted = r1.apply(lambda x:regex_pos_ag(branch_point_consensus,x)).values\n",
    "grouped = pd.Series(A3SS_data[AG_inserted]).groupby(r1[AG_inserted].apply(lambda x:regex_pos(branch_point_consensus,x)).values)\n",
    "bp_pos_data_AG = grouped.aggregate({'Mean':mean,'Size':lambda x:pd.notnull(x).sum()})\n",
    "grouped = pd.Series(A3SS_data[~AG_inserted]).groupby(r1[~AG_inserted].apply(lambda x:regex_pos(branch_point_consensus,x)).values)\n",
    "bp_pos_data_noAG = grouped.aggregate({'Mean':mean,'Size':lambda x:pd.notnull(x).sum()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "bp_pos_data_AG.Mean.plot(ax=ax,marker='^',linewidth=2,label='AG Inserted Between BP and SA',color='g')\n",
    "bp_pos_data_noAG.Mean[1:].plot(ax=ax,marker='o',linewidth=2,label='No AG Inserted Between BP and SA',color='b')\n",
    "ax.plot([-1,22.5],[bp_pos_data.Mean[-1]]*2,linewidth=2,color='r',label='No Consensus BP')\n",
    "axis([0,22.5,0,0.2])\n",
    "ax.set_xticks(range(-1,22,5));\n",
    "ax.set_xticklabels(range(-21-18-1,-18,5));\n",
    "ax.tick_params(labelsize=fsize)\n",
    "ax.set_xlabel('Branchpoint Position\\n(Relative to $SA_1$)',fontsize=fsize)\n",
    "ax.set_ylabel('$SA_1$ Usage (Fraction)',fontsize=fsize)\n",
    "leg = legend(numpoints=1,bbox_to_anchor=(2.78,1),fontsize=fsize)\n",
    "leg.get_frame().set_alpha(0)\n",
    "ax.grid('off')\n",
    "if SAVEFIGS:\n",
    "    filename = 'Branch_point_position_AG_exclusion'\n",
    "    fig.savefig(figdir+filename+'.png', bbox_inches='tight',dpi=300)\n",
    "    fig.savefig(figdir+filename+'.eps', bbox_inches='tight',dpi=200)\n",
    "    fig.savefig(figdir+filename+'.pdf', bbox_inches='tight',dpi=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
